{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Linear Classifiers\n",
    "\n",
    "This Jupyter notebook will cover the topics:\n",
    "\n",
    "* ID3 algorithm\n",
    "* Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "This notebook is a bit more code-heavy than the one before. But don't try to let that overwhelm you in case you're not that expecienced. The structure is already given, key parts are explained by comments, and only small bits inbetween need to be added.\n",
    "\n",
    "In terms of new programming concepts, this notebook makes use of functions and recursion. Below is a minimal explanatory showcase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function\n",
    "def add_one_and_double(x):\n",
    "    y = x + 1\n",
    "    z = y * 2\n",
    "    return z\n",
    "\n",
    "print(add_one_and_double(3))\n",
    "\n",
    "# A recursive function\n",
    "def factorial(n):\n",
    "    if n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)  # <-- here the function calls itself\n",
    "    \n",
    "print(factorial(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import log2 as log\n",
    "eps = np.finfo(float).eps  # a number very close to zero that we'll use further down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a data set on student performance in this exercise. If you want to learn more about the data, check the following link: https://archive.ics.uci.edu/ml/datasets/student+performance\n",
    "\n",
    "We're going to use only 6 attributes to make the result more illustrative. For the same reason we're renaming some values and columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.read_csv('school_grades_weka_dataset.csv')\n",
    "print(grades.shape)\n",
    "grades = grades.iloc[:,[1,2,3,4,20,32]]  # select columns only with given indexes\n",
    "grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the age attribute from numeric to binary. To find an appropriate split we're using the median and mean functions from the numpy package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(grades.age))\n",
    "print(np.mean(grades.age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like 17 is a good split point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = grades.rename(columns={\"G3\":'fin_grade'})\n",
    "\n",
    "#list comprehension to make the values more illustrative\n",
    "grades.address = [\"urban\" if x == 'U' else \"rural\" for x in grades.address]\n",
    "grades.famsize = [\">3\" if x == 'GT3' else \"<=3\" for x in grades.famsize]\n",
    "grades.sex = ['female' if x == 'F' else 'male' for x in grades.sex]\n",
    "grades.age = ['>=17' if x >= 17 else '<17' for x in grades.age ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.fin_grade = ['excellent' if x>=18 else 'very good' if x>=16 else 'good' if x>=14 else 'sufficient' if x>=10 else 'weak' if x>=4 else 'poor' for x in grades.fin_grade ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final attributes and their values:\n",
    "\n",
    "* sex - student's sex (binary: 'female' or 'male')\n",
    "* age - student's age (binary: '<17' or '>=17')\n",
    "* address - student's home address type (binary: 'urban' or 'rural')\n",
    "* famsize - family size (binary: '<=3' or '>3')\n",
    "* higher - wants to take higher education (binary: 'yes' or 'no')\n",
    "* fin_grade - final grade (nominal: 'excellent', 'very good', 'good', 'sufficient', 'weak', 'poor')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is a bit imbalanced with more than a half of final grades equal to 'sufficient'. We're adjusting the dataset manually to get a more illustrative example of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_wo_suff = grades[grades.fin_grade != 'sufficient']  # filter grades different from 'sufficient'\n",
    "subset_w_suff = grades[grades.fin_grade == 'sufficient'].sample(n = 90).reset_index(drop = True)  # filter 'sufficient' grades and keep only 90 rows\n",
    "grades = subset_w_suff.append(subset_wo_suff, ignore_index=True)  # merge two dataframes together\n",
    "grades = grades.sample(frac=1).reset_index(drop=True)  # shuffle the final dataframe\n",
    "print(grades.shape)\n",
    "grades.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Algorithm Implementation\n",
    "\n",
    "\n",
    "\n",
    "**Task:** Complete the code below to calculate entropy for a given dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(df):\n",
    "    \n",
    "    target = df.keys()[-1]  # get the name of target column\n",
    "    entropy = 0\n",
    "    target_values = df[target].unique()  # get unique values of the target column {'excellent', 'very good', ...}\n",
    "    \n",
    "    for value in target_values:\n",
    "        # df[column].value_counts()[value] returns number of rows with the specific value in the given column\n",
    "        relative_frequency_value = df[target].value_counts()[value] / len(df[target]) \n",
    "        entropy += # ...\n",
    "        \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Complete the code below to calculate entropy for a given dataframe and attribute\n",
    "\n",
    "*Note:* We're using eps to avoid getting log(0) or 0 in the denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_attribute(df, attribute):\n",
    "    \n",
    "    target = df.keys()[-1]  # get the name of target column\n",
    "    target_values = df[target].unique()  # get unique values of the target column {'excellent', 'very good', ...}\n",
    "    \n",
    "    attribute_values = df[attribute].unique()  # get unique values of the attribute column\n",
    "    average_entropy = 0\n",
    "    \n",
    "    for att_value in attribute_values:\n",
    "        \n",
    "        set_entropy = 0\n",
    "        \n",
    "        for target_value in target_values:\n",
    "            \n",
    "            num_elements_in_class = # ...\n",
    "            num_elements_in_set = # ...\n",
    "            relative_frequency = num_elements_in_class/(eps+num_elements_in_set) \n",
    "            set_entropyt += -relative_frequency*log(eps+relative_frequency)\n",
    "        \n",
    "        partition_weight = # ...\n",
    "        average_entropy += # ...\n",
    "        \n",
    "    return abs(average_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Complete the code below to calculate maximum information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_gain(df):\n",
    "    \n",
    "    gains = []\n",
    "    attributes = df.keys()[:-1]  # get names of all attributes columns\n",
    "    \n",
    "    for attribute in attributes:\n",
    "        \n",
    "        gain = # ...\n",
    "        gains.append(gain)\n",
    "    \n",
    "    return attributes[np.argmax(gains)], np.max(gains)  # np.argmax(array) returns index of max element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Complete the code below to build the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(df, tree=None):\n",
    "    \n",
    "    target = df.keys()[-1]\n",
    "    node, gain = find_max_gain(df) \n",
    "    attributes = np.unique(df[node])\n",
    "    \n",
    "    if tree is None:                    \n",
    "        tree={}\n",
    "        tree[node] = {}\n",
    "        \n",
    "    for value in attributes:\n",
    "        \n",
    "        subtable = df[df[node] == value].reset_index(drop=True) \n",
    "        subtable.drop(columns = [node], inplace = True)  # delete the current node column\n",
    "        \n",
    "        # np.unique() returns sorted unique elements of the array/column and their counts\n",
    "        clValue, counts = np.unique(subtable[target], return_counts=True)\n",
    "        \n",
    "        if len(counts)==1 or len(subtable.columns) == 1:  # Check if the node is the terminal/leaf node\n",
    "            tree[node][value] = clValue[#...]                                            \n",
    "        else:        \n",
    "            tree[node][value] = # ...\n",
    "                   \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pprint package to print the formatted representation of the tree that we save as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "tree = decision_tree(grades)\n",
    "    \n",
    "pprint.pprint(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Modify code above to build a decision tree with given maximum depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_with_max_depth(df, tree=None, max_depth=None):\n",
    "    \n",
    "    if max_depth is None:\n",
    "        #...\n",
    "    \n",
    "    if tree is None:                    \n",
    "        tree={}\n",
    "        tree[node] = {}\n",
    "        \n",
    "    for value in attributes:\n",
    "        \n",
    "        subtable = df[df[node] == value].reset_index(drop=True)\n",
    "        subtable.drop(columns = [node], inplace = True)  # delete the current node column\n",
    "        \n",
    "        # np.unique() returns sorted unique elements of the array/column and their counts\n",
    "        clValue,counts = np.unique(subtable[target], return_counts=True)   \n",
    "        \n",
    "        if len(counts)==1 or #...\n",
    "            tree[node][value] = clValue[#...]                                                    \n",
    "        else:        \n",
    "            tree[node][value] = #...\n",
    "                   \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_max_depth = decision_tree_with_max_depth(grades, max_depth = 3)\n",
    "    \n",
    "pprint.pprint(tree_max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "Perceptron is an algorithm for supervised learning of binary classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.linear_model.Perceptron\n",
    "\n",
    "At first we're going to look at the perceptron implementation in the sklearn package.\n",
    "\n",
    "Below code defines our data set and plots it using the matplotlib package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X1 = np.array([[ 0.33779625,  0.43771315],\n",
    "       [-3.04215519,  0.44362234],\n",
    "       [ 1.55633835,  1.50277908],\n",
    "       [-1.73490571,  1.6579759 ],\n",
    "       [-1.73168615,  1.49470015],\n",
    "       [ 0.8667018 ,  0.41225495],\n",
    "       [-2.87771733,  0.86954988],\n",
    "       [ 0.75223565,  0.08440232],\n",
    "       [-1.47945738,  1.2616705 ],\n",
    "       [-0.4785672 ,  1.19985036],\n",
    "       [ 1.62548382,  1.1795993 ],\n",
    "       [-0.84739376,  0.2028666 ],\n",
    "       [ 2.13363607,  1.14552905],\n",
    "       [ 0.27267319,  0.38029338],\n",
    "       [ 2.14503557,  0.22562339],\n",
    "       [ 1.36963359, -0.13479744],\n",
    "       [-0.41267018,  1.77212241],\n",
    "       [-2.9671909 ,  1.72121792],\n",
    "       [-1.32503909,  1.28815191],\n",
    "       [ 1.80576425,  0.99594517],\n",
    "       [-1.97679324,  0.27128576],\n",
    "       [-1.74128134,  2.56158409],\n",
    "       [ 1.58181139,  0.76777454],\n",
    "       [-0.21715638,  2.41168968],\n",
    "       [ 0.24130261,  0.68114926],\n",
    "       [ 2.93403797, -0.70272327],\n",
    "       [ 0.54918186, -0.9246533 ],\n",
    "       [-0.94476044,  0.79698854],\n",
    "       [-0.20668873, -0.29360169],\n",
    "       [ 2.07157736, -1.68353633],\n",
    "       [-1.32415501,  2.02986065],\n",
    "       [ 1.11469681,  0.88390513],\n",
    "       [-0.17081595,  2.95303183],\n",
    "       [ 1.81382203,  0.58587021],\n",
    "       [ 0.42420337,  0.87468868],\n",
    "       [-1.38599572,  1.79387082],\n",
    "       [ 0.56997773,  0.84988166],\n",
    "       [ 0.03764063,  0.70148957],\n",
    "       [ 0.85092623,  0.22867238],\n",
    "       [-0.82355759,  1.86393625]])\n",
    "Y1 = np.array([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, \n",
    "              1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1])\n",
    "\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1, s=20, edgecolor='k')  # plot the given points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppr = Perceptron()\n",
    "ppr.fit(X1, Y1)  # this is where the learning happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Complete the code below to find the weights and bias that were learned by the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = # ...\n",
    "weights = # ...\n",
    "bias, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to add the decision boundary learned by the perceptron to the plot of data points. Therefore we need to find the slope and intercept for the classes separation line.\n",
    "\n",
    "**Task:** Complete code below to find the intercept and slope\n",
    "\n",
    "*Note:* You need to find the line that passes through two points:\n",
    "* point_1 = ( 0 , - bias / weight_2 ) \n",
    "* point_2 = ( - bias / weight_1 , 0 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = # ...\n",
    "slope = # ...\n",
    "intercept, slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell plots the data points with the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1  # find the x-boundaries for the plot\n",
    "y_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1  # find the y-boundaries for the plot\n",
    "\n",
    "a = np.linspace(x_min, x_max,100)  # returns 100 evenly spaced samples, calculated over the interval [x_min, x_max]\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([y_min,y_max])  # set the y-boundaries\n",
    "axes.set_xlim([x_min,x_max])  # set the x-boundaries\n",
    "\n",
    "plt.plot(a, slope * a + intercept, 'b')  # plot the decision boundary\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1, s=20, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Training algorithm\n",
    "\n",
    "Now we're going to implement the Perceptron Training algorithm from scratch.\n",
    "\n",
    "We create data for classification using the make_blobs function from sklearn. As the data is created randomly, the classes will not always be linearly separable.\n",
    "\n",
    "For a start, execute below cell until you get two nicely separable blobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, Y = datasets.make_blobs(n_samples=100, centers=2, n_features=2, center_box=(0, 10))\n",
    "plt.plot(X[:, 0][Y == 0], X[:, 1][Y == 0], 'g^')  # plot the given points of first class\n",
    "plt.plot(X[:, 0][Y == 1], X[:, 1][Y == 1], 'bs')  # plot the given points of second class\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Complete the code below to implement Perceptron Training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(features, labels, learning_rate=1, max_num_iter=50):  \n",
    "    \n",
    "    # set weights and bias to zero\n",
    "    weights = np.zeros(shape = features.shape[1])\n",
    "    bias = 0\n",
    "    \n",
    "    num_iter = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        misclassified = 0  # set number of misclassified elements in this iteration to zero\n",
    "        \n",
    "        for i in range(len(features)):\n",
    "            x = features[i]  # get the features of the current point\n",
    "            \n",
    "            actual = labels[i]  # get the label of the current point\n",
    "            \n",
    "            predicted = # ...\n",
    "            \n",
    "            error = actual - predicted\n",
    "            \n",
    "            if predicted != actual:\n",
    "                \n",
    "                weights += # ...\n",
    "                bias += # ...\n",
    "                misclassified += # ...\n",
    "            \n",
    "        if max_num_iter <=20 :\n",
    "            print(f\" - NumIteration : {num_iter+1}. Missclassified : {misclassified}\" )\n",
    "        \n",
    "        num_iter += 1\n",
    "                  \n",
    "        if misclassified == 0 or num_iter >= max_num_iter:\n",
    "            break\n",
    "            \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = perceptron(X,Y)\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Use the formula for intercept and slope from the example above to plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = # ...\n",
    "slope = # ...\n",
    "intercept, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1  # find the x-boundaries for the plot\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1  # find the y-boundaries for the plot\n",
    "\n",
    "a = np.linspace(x_min, x_max,100)  # returns 100 evenly spaced samples, calculated over the interval [x_min, x_max]\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([y_min,y_max])  # set the y-boundaries\n",
    "axes.set_xlim([x_min,x_max])  # set the x-boundaries\n",
    "\n",
    "plt.plot(a, slope * a + intercept, 'b')  # plot the decision boundary\n",
    "plt.plot(X[:, 0][Y == 0], X[:, 1][Y == 0], 'g^')\n",
    "plt.plot(X[:, 0][Y == 1], X[:, 1][Y == 1], 'bs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Re-run the blob creation cell again until you get two blobs with very high overlap. Running the perceptron once more and looking at the decision boundary, it will probably look somewhat (or ever completely) off. Tweak your perceptron function to achieve a better decision boundary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
