{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of KD Experiments\n",
    "========================\n",
    "\n",
    "This Jupyter notebook will cover the topics:\n",
    "\n",
    "* precision and recall\n",
    "* nDCG\n",
    "* confidence intervals\n",
    "\n",
    "Precision and Recall\n",
    "-----------------------------\n",
    "\n",
    "Assume you trained a model to detect cats in images. The two lists below contain the true labels for a sequence of 20 images (`true_labels`) and the predictions of your model (`predicted_labels`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [\n",
    "    'cat',\n",
    "    'cat',\n",
    "    'no_cat',\n",
    "    'cat',\n",
    "    'no_cat',\n",
    "    'cat',\n",
    "    'cat',\n",
    "    'no_cat',\n",
    "    'cat',\n",
    "    'no_cat',\n",
    "    'no_cat',\n",
    "    'cat',\n",
    "    'cat',\n",
    "    'no_cat',\n",
    "    'cat',\n",
    "    'cat',\n",
    "    'cat',\n",
    "    'cat',\n",
    "    'no_cat',\n",
    "    'cat'\n",
    "]\n",
    "predicted_labels = [\n",
    "    'cat',\n",
    "    'no_cat',\n",
    "    'no_cat',\n",
    "    'no_cat',\n",
    "    'no_cat',\n",
    "    'cat',\n",
    "    'no_cat',\n",
    "    'cat',\n",
    "    'cat',\n",
    "    'no_cat',\n",
    "    'no_cat',\n",
    "    'cat',\n",
    "    'cat',\n",
    "    'no_cat',\n",
    "    'cat',\n",
    "    'cat',\n",
    "    'no_cat',\n",
    "    'cat',\n",
    "    'cat',\n",
    "    'cat'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Complete the code below to calculate precision and recall for the given predictions.\n",
    "\n",
    "*Note:* You're implementing the calculation as an exercise. In real applications one would rely on a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predictions = len(predicted_labels)\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for i in range(num_predictions):\n",
    "    ground_truth = true_labels[i]\n",
    "    prediction = predicted_labels[i]\n",
    "    # ...\n",
    "        \n",
    "precision = # ...\n",
    "recall = # ...\n",
    "\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume your model outputs its confidence of having detected a cat as a probability score $\\in [0-1]$. For above predicted labels, a threshold of 0.5 was used.\n",
    "\n",
    "**Task:** Complete the code below to generate a precision recall curve.\n",
    "\n",
    "*Note:* From hereon we'll use methods from the *sklearn* library to calculate precision and recall. As the function expects *0*s and *1*s as class \"labels\" we have to convert the ground truth accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "prediction_probabilities = [\n",
    "    0.71,  # cat\n",
    "    0.19,  # no_cat\n",
    "    0.20,  # no_cat\n",
    "    0.46,  # no_cat\n",
    "    0.44,  # no_cat\n",
    "    0.61,  # cat\n",
    "    0.22,  # no_cat\n",
    "    0.80,  # cat\n",
    "    0.82,  # cat\n",
    "    0.41,  # no_cat\n",
    "    0.15,  # no_cat\n",
    "    0.82,  # cat\n",
    "    0.89,  # cat\n",
    "    0.47,  # no_cat\n",
    "    0.75,  # cat\n",
    "    0.57,  # cat\n",
    "    0.40,  # no_cat\n",
    "    0.87,  # cat\n",
    "    0.67,  # cat\n",
    "    0.71   # cat\n",
    "]\n",
    "\n",
    "# convert ground truth labels to numerical values\n",
    "cat_to_num = {\n",
    "    'cat': 1,\n",
    "    'no_cat': 0\n",
    "}\n",
    "ground_truths_numerical = [\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# calculate\n",
    "precision_values = []\n",
    "recall_values = []\n",
    "\n",
    "for i in range(101):\n",
    "    threshold = i/100\n",
    "    \n",
    "    # determine predicted classes (as numerical values)\n",
    "    predictions_numerical = []\n",
    "    # ...\n",
    "    \n",
    "    precision = precision_score(\n",
    "        ground_truths_numerical,\n",
    "        predictions_numerical,\n",
    "        average='binary',\n",
    "        pos_label=1,\n",
    "        zero_division=1\n",
    "    )\n",
    "    recall = recall_score(\n",
    "        ground_truths_numerical,\n",
    "        predictions_numerical,\n",
    "        average='binary',\n",
    "        pos_label=1,\n",
    "        zero_division=1\n",
    "    )\n",
    "    \n",
    "    # ...\n",
    "\n",
    "plt.plot(precision_values, recall_values, marker='.')\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your output against a precision recall curve generated by *sklearn*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "p_vals_check, r_vals_check, thresholds = precision_recall_curve(\n",
    "    ground_truths_numerical,\n",
    "    prediction_probabilities\n",
    ")\n",
    "plt.plot(p_vals_check, r_vals_check, marker='.')\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly you can, for example, easily generate a ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(ground_truths_numerical, prediction_probabilities)\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nDCG\n",
    "---------\n",
    "\n",
    "In the evaluation of a recommender system you got an ordered list of 20 documents with the following relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_scores = [\n",
    "    0.7, 0.9, 0.3, 0.5, 0.1, 0.1, 0.8, 0.2, 0.4, 0, 0, 0, 0, 0, 0.1, 0.5, 0, 0.3, 0, 0.1\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Complete the implementation of the nDCG score below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "num_documents = len(relevance_scores)\n",
    "ideal_order_scores = sorted(relevance_scores, reverse=True)\n",
    "\n",
    "dcg = 0\n",
    "idcg = 0\n",
    "\n",
    "for i in range(num_documents):\n",
    "    # ...\n",
    "\n",
    "ndcg = # ...\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Copy and modify above code such that the nDCG@5 is calculated.\n",
    "\n",
    "* a) with a focus purely on the *order* of the top 5 documents\n",
    "* b) with a focus on the result quality in general evaluated at by means of the top 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Order focus\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) General result quality focus\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence intervals\n",
    "-----------------------------\n",
    "\n",
    "To get information on the basic properties of of a list of values, we can use the *Pandas* library. Working with *Pandas* you will often encounter DataFrames (you can think of these as tables). For a list of values (you can also think of a column in a table), *Pandas* uses so called Series.\n",
    "\n",
    "We'll start out by defining a standard Python list and creating a Series from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "l = [1, 4, 3, 4, 0, 6]\n",
    "s = pd.Series(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Execute below cell and take a look at `s`.\n",
    "\n",
    "You'll see that every element as a number (called *index*) associated with it. You can access a single element of `s` using `s.at[i]` where `i` is the index of the value.\n",
    "\n",
    "`s` has *a lot* more to offer than just `.at`. You can use Python's builtin function `dir()` to see the list of all the attributes of the Series `s` (enter `dir(s)` and execute). *Note:* attributes starting with an underscore are typically meant for internal use of the class and not for use from \"outside\".\n",
    "\n",
    "**Task:** pick a few interesting/useful sounding attributes of `s` and inform yourself about them using `help(s.foo)` where `foo` is the attributes name. Showcase the use of two attributes and print some useful/interesting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the values related to confidence inverfals are directly available as attributes provided by Series objects. These are`var` (variance), `std` (standard deviation),  and `sem` (standard error).\n",
    "\n",
    "Let's assume the values in `l` are observations in an experiment. We can get their variance, standard deviation and standard error as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_deg_of_freedom = 0  # not using Bessel's correction (done here just so that we get nice numbers)\n",
    "\n",
    "variance = s.var(ddof=delta_deg_of_freedom)\n",
    "std = s.std(ddof=delta_deg_of_freedom)\n",
    "std_err = s.sem(ddof=delta_deg_of_freedom)\n",
    "\n",
    "print(f'Variance: {variance}')\n",
    "print(f'Standard deviation: {std}')\n",
    "print(f'Standard error: {std_err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Complete the code below to calculate a confidence interval for `s`.\n",
    "\n",
    "*Note:* You may use `std` but not `sdr_err`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "n = len(s)  # number of observations\n",
    "confidence = 0.95\n",
    "students_t = t.ppf((1 + confidence) / 2, n)\n",
    "print('Student\\'s t = {:.4f}'.format(students_t))\n",
    "\n",
    "std_error = # ...\n",
    "# ...\n",
    "upper_bound = # ...\n",
    "lower_bound = # ...\n",
    "print(f'Confidence interval (95%): {lower_bound:.4f} ≤ μ ≤ {upper_bound:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice way to visualize a series of measurements for a given value is a box plot. Below cell generates 50 random values and generates a box blot with nochtes.\n",
    "\n",
    "**Task:** Execute below cell and identify the informative elements that appear in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random_vals = np.random.normal(0, 2, 50)\n",
    "random_vals[0] = 7\n",
    "df_rand_vals = pd.DataFrame(random_vals, columns=['vals'])\n",
    "\n",
    "print('Random values: {}'.format(\n",
    "    ','.join(['{:.2f}'.format(v) for v in random_vals])\n",
    "))\n",
    "print('Median: {}'.format(\n",
    "    df_rand_vals.vals.median()\n",
    "))\n",
    "print('95% confidence interval: {}'.format(\n",
    "    t.interval(\n",
    "        0.95,\n",
    "        len(foo),\n",
    "        loc=df_rand_vals.vals.mean(),\n",
    "        scale=df_rand_vals.vals.sem()\n",
    "    )\n",
    "))\n",
    "df_rand_vals.boxplot(notch=True, figsize=(15,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
