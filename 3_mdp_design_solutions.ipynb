{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Setup\n",
    "\n",
    "If you prefer to work locally, see the following instructions for setting up Python in a virtual environment.\n",
    "You can then ignore the instructions in \"Colab Setup\".\n",
    "\n",
    "If you haven't yet, create a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) environment using:\n",
    "```\n",
    "conda create --name rl_exercises\n",
    "conda activate rl_exercises\n",
    "```\n",
    "Torch recommends installation using conda rather than pip, so run:\n",
    "```\n",
    "conda install pytorch cpuonly -c pytorch\n",
    "```\n",
    "If you have a CUDA-enabled GPU and would like to use it, visit [the installation page](https://pytorch.org/get-started/locally/) to see the options available for different CUDA versions.\n",
    "The remaining dependencies can be installed with pip:\n",
    "```\n",
    "pip install ipykernel matplotlib numpy \"gymnasium[classic-control, mujoco]\" stable_baselines3\n",
    "```\n",
    "\n",
    "Even if you are running the Jupyter notebook locally, please run the code cells in **Colab Setup**, because they define some global variables required later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup\n",
    "\n",
    "Google Colab provides you with a temporary environment for python programming.\n",
    "While this conveniently works on any platform and internally handles dependency issues and such, it also requires you to set up the environment from scratch every time.\n",
    "The \"Colab Setup\" section below will be part of **every** exercise and contains utility that is needed before getting started.\n",
    "\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "Any changes you make to the Jupyter notebook itself should be saved to your Google Drive.\n",
    "We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    "However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Your work will be stored in a folder called `rl_ws24` by default to prevent Colab \n",
    "instance timeouts from deleting your edits.\n",
    "We do this by mounting your google drive on the virtual machine created in this colab \n",
    "session. For this, you will likely need to sign in to your Google account and allow\n",
    "access to your Google Drive files.\n",
    "\"\"\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    COLAB = True\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "\n",
    "# Create paths in your google drive\n",
    "if COLAB:\n",
    "    DATA_ROOT = Path(\"/content/gdrive/My Drive/rl_ws24\")\n",
    "    DATA_ROOT.mkdir(exist_ok=True)\n",
    "else:\n",
    "    DATA_ROOT = Path.cwd() / \"rl_ws24\"\n",
    "\n",
    "# Install **python** packages\n",
    "if COLAB:\n",
    "    %pip install matplotlib numpy \"gymnasium[mujoco, classic-control, other]\" stable_baselines3 torch\n",
    "\n",
    "    # Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "    # This is usually installed as part of an Nvidia driver package, but the Colab\n",
    "    # kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
    "    # (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "    NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "    import os\n",
    "    if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "      with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "        f.write(\"\"\"{\n",
    "        \"file_format_version\" : \"1.0.0\",\n",
    "        \"ICD\" : {\n",
    "            \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "        }\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "    # Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "    print('Setting environment variable to use GPU rendering:')\n",
    "    %env MUJOCO_GL=egl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: MDP Design\n",
    "\n",
    "While the lecture focusses mainly on algorithmic developments, or the strengths and weaknesses of different methods, the environments those algorithms are applied to are usually taken for granted.\n",
    "However, when trying to apply RL algorithms to a novel real-world problem, we often have many design choices to make that drastically affect the final outcome.\n",
    "Designing a reward function, observations, actions, and termination conditions for a given task is [a famously frustrating and difficult problem](https://www.alexirpan.com/2018/02/14/rl-hard.html).\n",
    "In this exercise, we will guide you through some of the pitfalls of applying RL in practice.\n",
    "\n",
    "\n",
    "All homeworks are self-contained.\n",
    "They can be completed in their respective notebooks.\n",
    "Please fill in any missing code or answer any questions that are marked with `## TODO ##` statements.\n",
    "Questions not marked with `## TODO ##` are self-test questions and do **not** need to be answered for points.\n",
    "To edit and re-run code, you can simply edit and restart the code cells below.\n",
    "When you are finished, you will need to submit the notebook as well as all saved figures (see exercises) as a zip file via Ilias.\n",
    "\n",
    "We start by importing all the necessary python modules and defining some helper functions which you do not need to change.\n",
    "Still, make sure you are aware of what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.core import Wrapper\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from IPython import display\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.logger import KVWriter, Logger\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "SEED = 3\n",
    "\n",
    "# specify the path to save the recordings of this run to.\n",
    "DATA_PATH = DATA_ROOT / \"exercise_3\" / time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "\n",
    "def save_figure(fig, save_name: str) -> None:\n",
    "    \"\"\"Saves a figure into your google drive folder or local directory\"\"\"\n",
    "    DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    path = DATA_PATH / (save_name + \".png\")\n",
    "    fig.savefig(str(path))\n",
    "\n",
    "\n",
    "class JupyterRenderEvalCallback(EvalCallback):\n",
    "    \"\"\"Renders evaluation trajectories in a matplotlib figure in a jupyter\n",
    "    notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_env, *args, ax, image, n_episodes_to_render=1, **kwargs):\n",
    "        super().__init__(eval_env, *args, **kwargs)\n",
    "        self.ax = ax\n",
    "        self.image = image\n",
    "        self.n_episodes_to_render = n_episodes_to_render\n",
    "\n",
    "    def _log_success_callback(self, locals_: dict, globals_: dict) -> None:\n",
    "        \"\"\"This function is called on every step of the evaluation, so we\n",
    "        hijack it for rendering the policy.\n",
    "        \"\"\"\n",
    "        # call the parent method because it's responsible for logging success\n",
    "        super()._log_success_callback(locals_, globals_)\n",
    "\n",
    "        # we use the 0th element here because there is only 1 eval env\n",
    "        if locals_[\"episode_counts\"][0] < self.n_episodes_to_render:\n",
    "            env = locals_[\"env\"]\n",
    "            rendering = env.render()\n",
    "            display.clear_output(wait=True)\n",
    "            self.image.set_data(rendering)\n",
    "            plt.sca(self.ax)\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "    def set_plt_refs(self, ax, image):\n",
    "        self.ax = ax\n",
    "        self.image = image\n",
    "\n",
    "\n",
    "class PlotFormat(KVWriter):\n",
    "    \"\"\"Logs values by plotting them in a matplotlib figure in a jupyter\n",
    "    notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, axs: list, lines: dict):\n",
    "        self.axs = axs\n",
    "        self.lines = lines\n",
    "        self.timesteps = []\n",
    "        self.data = defaultdict(list)\n",
    "\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: dict,\n",
    "        key_excluded={},\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "        if any(key in key_values for key in self.lines):\n",
    "            assert all(key in key_values for key in self.lines)\n",
    "            self.timesteps.append(step)\n",
    "            display.clear_output(wait=True)\n",
    "            for key, line in self.lines.items():\n",
    "                self.data[key].append(key_values[key])\n",
    "                line.set_data(self.timesteps, self.data[key])\n",
    "            for ax in self.axs:\n",
    "                ax.relim()  # Recompute the data limits\n",
    "                ax.autoscale_view()  # Autoscale to fit new data\n",
    "            plt.sca(self.axs[0])\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "    def set_plt_refs(self, axs: list, lines: dict):\n",
    "        self.axs = axs\n",
    "        self.lines = lines\n",
    "\n",
    "    def clear(self):\n",
    "        self.timesteps.clear()\n",
    "        self.data.clear()\n",
    "\n",
    "\n",
    "def create_figure(figsize: tuple[int, int], n_steps: int) -> tuple:\n",
    "    fig, axs = plt.subplots(1, 3, figsize=tuple(figsize))\n",
    "    axs = {\"render\": axs[0], \"return\": axs[1], \"success\": axs[2]}\n",
    "\n",
    "    # remove tickmarks and axis labels\n",
    "    axs[\"render\"].set_axis_off()\n",
    "\n",
    "    axs[\"return\"].set(title=\"Mean Return\", xlabel=\"Environment Steps\")\n",
    "    axs[\"return\"].set_xlim(0, n_steps)\n",
    "\n",
    "    axs[\"success\"].set(\n",
    "        title=\"Success Rate\", xlabel=\"Environment Steps\", ylabel=\"Success\"\n",
    "    )\n",
    "    axs[\"success\"].set_xlim(0, n_steps)\n",
    "    axs[\"success\"].set_ylim(0, 1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Termination vs. Truncation (2 points)\n",
    "\n",
    "First we will look at a common mistake that can subtly break many RL algorithms.\n",
    "Adding to the difficulty is that many algorithms will usually still learn, they will just perform slightly worse.\n",
    "We demonstrate this mistake on SAC, because it tends to be the most sensitive to it.\n",
    "Understanding this problem will help you learn how to visualize an MDP from the agent's \"perspective\".\n",
    "\n",
    "The gymnasium `step` function returns two booleans that both indicate the end of a trajectory: `terminated` and `truncated`.\n",
    "\n",
    "`terminated` is a feature of the MDP itself.\n",
    "It describes when a terminal state of the MDP has been reached, and no more future rewards are possible.\n",
    "Intuitively, we can imagine that the agent has either \"lost\" or succeeded, and the \"game\" is over.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media1.tenor.com/m/Kzd7ERPff_gAAAAC/game-over-game.gif\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "`truncated` means that a time limit has been reached, but the MDP is not necessarily in a terminal state.\n",
    "The time limit is usually added as a part of training to regularly \"teleport\" the agent back to the starting state.\n",
    "This prevents too much useless data from being generated, especially at the start of training, where the agent wanders somewhere far from the optimal trajectory and can't find its way back.\n",
    "However, the agent could have continued collecting rewards if we had let it.\n",
    "Future rewards would have been possible.\n",
    "\n",
    "\\## SOLUTION ##\n",
    "\n",
    "The terminated signal is used in the computation of the targets for value learning or Q-Learning, e.g.:\n",
    "\\begin{equation}\n",
    "    y(r,s',d) = r + \\gamma (1 - d_{\\mathrm{ter}}) \\left( \\mathrm{max}_{a'} Q (s',a') \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Other similar equations, e.g. for continuous Q values (like SAC) or value learning (like PPO) are also accepted.\n",
    "\n",
    "The truncated signal is not used for learning, only for resetting the environment.\n",
    "\n",
    "\\## END ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The code cell below contains a script for training a policy to solve Pendulum with SAC using the [Stable Baselines 3](https://stable-baselines3.readthedocs.io) framework.\n",
    "We define a success metric using an [environment wrapper](https://gymnasium.farama.org/api/wrappers/) that writes a boolean into the `info` dict returned by the step function when a trajectory ends.\n",
    "Of course, Stable Baselines 3 does handle truncation and termination correctly, but we can simulate the bug we are interested in by modifying the environment itself to return incorrect values.\n",
    "This is done by adding an environment wrapper that incorrectly treats timeouts as terminations and modifies the results of the environment's `step` function.\n",
    "Fill in the missing code in the `step` function of the `TruncateAsTerminateWrapper`.\n",
    "Then, run the code cell to see the effect that this error can have on SAC.\n",
    "\n",
    "In the figure below, you can watch the policy evaluations in the leftmost subplot.\n",
    "Only the first evaluation trajectory is shown, but the metrics on the right are averaged over 20 trajectories that run without rendering.\n",
    "The other plots show learning curves for mean return (sum of rewards) and success for each variant.\n",
    "\n",
    "Warning! You should never attempt to learn about Reinforcement Learning by reading the source code of Stable Baselines 3.\n",
    "Exposure to the source code of SB3, even for short periods, has been shown to cause long-lasting health effects.\n",
    "To actually learn about RL, we recommend [cleanrl](https://cleanrl.dev)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PendulumSuccessWrapper(Wrapper):\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        if truncated or terminated:\n",
    "            x, y, omega = obs\n",
    "            theta = np.arctan2(y, x) * 180 / np.pi\n",
    "            info[\"is_success\"] = abs(theta) < 10 and abs(omega) < 1e-2\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class TruncateAsTerminateWrapper(Wrapper):\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "        ## SOLUTION ##\n",
    "        # change the return values so that the algorithm treats truncation identically to termination\n",
    "        terminated = terminated or truncated\n",
    "        ## END ##\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "def make_pendulum(max_episode_steps=None, incorrect=False, **kwargs):\n",
    "    env = gym.make(\"Pendulum-v1\", max_episode_steps=max_episode_steps, **kwargs)\n",
    "    env = PendulumSuccessWrapper(env)\n",
    "    if incorrect:\n",
    "        env = TruncateAsTerminateWrapper(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "SEED = 3\n",
    "N_STEPS = 5000\n",
    "LOG_FREQ = EVAL_FREQ = 500\n",
    "RENDER = False\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "sb3_seed = int(rng.integers(2**32))\n",
    "eval_env_seed = int(rng.integers(2**32))\n",
    "\n",
    "# we always evaluate on the original environment, although we might train\n",
    "# on an improved version of the environment\n",
    "eval_env = make_pendulum(render_mode=\"rgb_array\")\n",
    "eval_env.reset()  # need to reset before rendering\n",
    "log_interval = LOG_FREQ / eval_env.get_wrapper_attr(\"_max_episode_steps\")\n",
    "\n",
    "fig, axs = create_figure(figsize=(12, 5), n_steps=N_STEPS)\n",
    "fig.suptitle(f\"Q1: Termination vs. Truncation\")\n",
    "fig.tight_layout()\n",
    "image = axs[\"render\"].imshow(eval_env.render())\n",
    "plotter = PlotFormat(axs=[], lines={})\n",
    "logger = Logger(folder=None, output_formats=[plotter])\n",
    "\n",
    "# NOTE: Comment out any variants you don't want to test\n",
    "VARIANTS = [\n",
    "    (\"timeout handled incorrectly\", True, \"red\"),\n",
    "    (\"timeout handled correctly\", False, \"navy\"),\n",
    "]\n",
    "for label, incorrect, color in VARIANTS:\n",
    "\n",
    "    env = make_pendulum(incorrect=incorrect, max_episode_steps=50)\n",
    "\n",
    "    # we don't change eval_env, because we test on the original problem\n",
    "    eval_env.reset(seed=eval_env_seed)\n",
    "    eval_callback = JupyterRenderEvalCallback(\n",
    "        eval_env,\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        n_eval_episodes=20,\n",
    "        n_episodes_to_render=1 if RENDER else 0,\n",
    "        ax=axs[\"render\"],\n",
    "        image=image,\n",
    "    )\n",
    "    plotter.clear()\n",
    "    plotter.set_plt_refs(\n",
    "        axs=[axs[\"return\"], axs[\"success\"]],\n",
    "        lines={\n",
    "            \"eval/mean_reward\": axs[\"return\"].plot([], [], color=color, label=label)[0],\n",
    "            \"eval/success_rate\": axs[\"success\"].plot([], [], color=color)[0],\n",
    "        },\n",
    "    )\n",
    "    axs[\"return\"].legend()\n",
    "\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        seed=sb3_seed,\n",
    "    )\n",
    "    model.set_logger(logger)\n",
    "    model.learn(\n",
    "        total_timesteps=N_STEPS, log_interval=log_interval, callback=eval_callback\n",
    "    )\n",
    "\n",
    "# clear duplicate plot\n",
    "display.clear_output()\n",
    "\n",
    "save_figure(fig, \"Q1_timeouts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Solution:\n",
    "\n",
    "![Solution](figures/E3_Q1.png)\n",
    "\n",
    "Using default timeout of 200 instead of 50:\n",
    "\n",
    "![Solution](figures/E3_Q1_longer-timeout.png)\n",
    "\n",
    "\n",
    "### Self-Test Questions (optional)\n",
    "\n",
    "In this question, we use SAC to demonstrate the loss in performance associated with incorrectly handling timeouts.\n",
    "Although this mistake should affect almost all algorithms (anything that learns a Q or value function), empirically, SAC is the most sensitive to it.\n",
    "Can you explain why?\n",
    "\n",
    "Answer: \n",
    "Policy gradient algos like PPO only use the value as a baseline, and so are not really affected if the baseline is slightly off.\n",
    "DQN (for discrete problems) is affected by this change, but the policy is only a result of the relative Q values of each action, which is less sensitive to small changes in Q.\n",
    "In contrast, SAC relies not just on accurate Q estimates but on accurate gradients of Q estimates.\n",
    "\n",
    "\n",
    "We set the timeout for the training environment to 50 steps but leave that of the evaluation environment at 200 steps.\n",
    "Why do each of these decisions help to amplify the effect we are trying to show here?\n",
    "\n",
    "Answer: \n",
    "If the timeout is 50 steps, then every 50th time step will result in a termination.\n",
    "The more frequent these unexpected terminations are (unexpected from the perspective of the agent), the stronger the effect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic vs. Infinite-Horizon Tasks\n",
    "\n",
    "There are two fundamental types of tasks in RL, although there is not really a strict definition for what they are.\n",
    "Their main distinguihsing feature is the `termination` condition (recall that almost all tasks have a truncation condition when a timeout is reached).\n",
    "\n",
    "Episodic tasks (also called finite-horizon tasks) are usually those where the goal is to change in the environment in some way or reach some state, after which the task is over.\n",
    "Examples may include Go, chess, navigation, pick-and-place, and many others.\n",
    "Infinite-horizon tasks (also called continuing tasks) usually involve trying to reach a goal state which is typically unstable and then staying there despite disturbances.\n",
    "They may also involve maximizing some utility in a way that cannot be easily divided or bounded into episodes.\n",
    "Examples may include plant control, walking or running, balancing things, trading stocks, and many others.\n",
    "\n",
    "The difficulty is that many tasks can be formulated as either episodic or infinite-horizon, and it is not immediately clear which formulation is easier for an algorithm learn.\n",
    "Reward design is often quite different depending on whether an episodic or an infinite-horizon formulation is chosen.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/mikelma/craftium/raw/main/craftium-docs/imgs/env_speleo.gif\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "For example, would you say this task is episodic or infinite-horizon?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Episodic Environments (3 points)\n",
    "\n",
    "Let's start with a modified version of an episodic environment you are probably already familiar with, [FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "However, this time the environment is deterministic (no slippery surfaces!), and instead we are simply trying to navigate to the goal without colliding with any obstacles (the ice patches).\n",
    "This environment will act as a stand-in for a more complex, 3D navigation problem.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://gymnasium.farama.org/_images/frozen_lake.gif\" width=\"200\"/>\n",
    "</center>\n",
    "\n",
    "A central theme of this exercise is the discrepancy between rewards and success.\n",
    "Success is often easy to define, and is a sparse metric that indicates if the desired state was reached at the end of a trajectory.\n",
    "Ideally, we could set our reward function to be our success metric:\n",
    "\n",
    "\\begin{equation}\n",
    "    r(s,a,s') = \\mathbf{I}(s' = s_{\\mathrm{goal}})\n",
    "\\end{equation}\n",
    "\n",
    "Unfortunately, this is usually too sparse for most real-world problems.\n",
    "If the agent does not find the goal state through random chance, it will never even figure out what the goal state is!\n",
    "Sparse rewards can work in 2D, but are usually impossible in 3D.\n",
    "Because there are so many more possible states in 3D, we need to give our agent shaped rewards so it feels a constant \"pull\" in the direction we want it to go.\n",
    "\n",
    "Therefore, we modify the rewards of FrozenLake to be dense (or shaped) rewards.\n",
    "Read the code for the `step` function yourself and verify that it makes sense.\n",
    "The success metric is simply whether the agent is in the goal state at the end of the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.toy_text.frozen_lake import DOWN, LEFT, RIGHT, UP, FrozenLakeEnv\n",
    "\n",
    "\n",
    "class ShapedRewardsFrozenLakeEnv(FrozenLakeEnv):\n",
    "    \"\"\"FrozenLake with dense rewards for navigating to goal.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, is_slippery=False, **kwargs):\n",
    "        super().__init__(*args, is_slippery=is_slippery, **kwargs)\n",
    "\n",
    "        nrow, ncol = self.nrow, self.ncol\n",
    "        desc = self.desc\n",
    "        self.goal = np.argmax(self.desc.flatten() == b\"G\")\n",
    "        self.goal_row, self.goal_col = np.divmod(self.goal, ncol)\n",
    "\n",
    "        def to_s(row, col):\n",
    "            return row * ncol + col\n",
    "\n",
    "        def inc(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col - 1, 0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row + 1, nrow - 1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col + 1, ncol - 1)\n",
    "            elif a == UP:\n",
    "                row = max(row - 1, 0)\n",
    "            return (row, col)\n",
    "\n",
    "        def update_probability_matrix(row, col, action):\n",
    "            new_row, new_col = inc(row, col, action)\n",
    "            new_state = to_s(new_row, new_col)\n",
    "            new_letter = desc[new_row, new_col]\n",
    "            terminated = bytes(new_letter) in b\"GH\"\n",
    "            reward = float(new_letter == b\"G\")\n",
    "            return new_state, reward, terminated\n",
    "\n",
    "        # modify MDP transitions so that agent can exit hole states\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = self.P[s][a]\n",
    "                    li.clear()\n",
    "                    if is_slippery:\n",
    "                        for b in [(a - 1) % 4, a, (a + 1) % 4]:\n",
    "                            li.append(\n",
    "                                (1.0 / 3.0, *update_probability_matrix(row, col, b))\n",
    "                            )\n",
    "                    else:\n",
    "                        li.append((1.0, *update_probability_matrix(row, col, a)))\n",
    "\n",
    "    def _get_square_type(self):\n",
    "        \"\"\"Returns the letter code (S, F, H, or G) for the current state.\n",
    "        S: Start\n",
    "        F: normal cell\n",
    "        H: hole (i.e. ice patch)\n",
    "        G: Goal\n",
    "        \"\"\"\n",
    "        return self.desc.flatten()[self.s]\n",
    "\n",
    "    def step(self, a):\n",
    "        obs, reward, terminated, truncated, info = super().step(a)\n",
    "\n",
    "        # modify rewards to be the negative distance to goal\n",
    "        row, col = divmod(obs, self.ncol)\n",
    "        distance = np.linalg.norm([self.goal_row - row, self.goal_col - col], ord=1)\n",
    "        reward = -distance / 10\n",
    "\n",
    "        # add rewards for reaching goal and hitting obstacle\n",
    "        square = self._get_square_type()\n",
    "        if square == b\"G\":\n",
    "            reward += 1\n",
    "        elif square == b\"H\":\n",
    "            reward += -1\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class FrozenLakeSuccessWrapper(Wrapper):\n",
    "    \"\"\"Adds a success metric to FrozenLake. A trajectory is successful if it\n",
    "    ends on the goal square.\n",
    "    \"\"\"\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        if truncated or terminated:\n",
    "            info[\"is_success\"] = obs == self.env.unwrapped.goal\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a training pipeline for PPO on our `ShapedRewardsFrozenLakeEnv` environment.\n",
    "Since this is a tabular environment, we define the value function and policy to be look-up tables (no hidden layers or non-linearities).\n",
    "\n",
    "Run the cell to train the policy.\n",
    "Don't worry if it looks like your agent isn't exploring.\n",
    "Only the deterministic evaluation trajectories are rendered, while the stochastic exploration steps are never shown.\n",
    "\n",
    "Notice that the returns seem to increase and then converge, but the success rate is always 0..\n",
    "Looking at the rendering, the policy seems to think it's best to throw itself into the nearest ice patch it can find.\n",
    "Can you think of why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frozenlake(max_episode_steps=100, fix_no=None, **kwargs):\n",
    "    if fix_no is not None:\n",
    "        EnvCls = FixedFrozenLakeEnv\n",
    "        kwargs[\"fix_no\"] = fix_no\n",
    "    else:\n",
    "        EnvCls = ShapedRewardsFrozenLakeEnv\n",
    "    env = EnvCls(**kwargs)\n",
    "    env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    env = FrozenLakeSuccessWrapper(env)\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "DISCOUNT = 0.99  # default discount value for PPO\n",
    "N_STEPS = 50000\n",
    "LOG_FREQ = EVAL_FREQ = 5000\n",
    "MAP_NAME = \"8x8\"\n",
    "RENDER = True\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "sb3_seed = int(rng.integers(2**32))\n",
    "eval_env_seed = int(rng.integers(2**32))\n",
    "\n",
    "# we always evaluate on the original environment, although we might train\n",
    "# on an improved version of the environment\n",
    "eval_env = make_frozenlake(map_name=MAP_NAME, render_mode=\"rgb_array\")\n",
    "eval_env.reset()  # need to reset before rendering\n",
    "log_interval = LOG_FREQ / eval_env.get_wrapper_attr(\"_max_episode_steps\")\n",
    "\n",
    "fig, axs = create_figure(figsize=(12, 5), n_steps=N_STEPS)\n",
    "fig.suptitle(f\"Q2: Episodic Tasks\")\n",
    "fig.tight_layout()\n",
    "image = axs[\"render\"].imshow(eval_env.render())\n",
    "plotter = PlotFormat(axs=[], lines={})\n",
    "logger = Logger(folder=None, output_formats=[plotter])\n",
    "display.display(plt.gcf())\n",
    "\n",
    "# NOTE: Comment out any variants you don't want to test\n",
    "VARIANTS = [\n",
    "    (\"original\", None, \"red\"),\n",
    "    # (\"fix #1\", 1, \"navy\"),\n",
    "    # (\"fix #2\", 2, \"aqua\"),\n",
    "    # (\"fix #3\", 3, \"green\"),\n",
    "]\n",
    "for label, fix_no, color in VARIANTS:\n",
    "\n",
    "    env = make_frozenlake(map_name=MAP_NAME, fix_no=fix_no)\n",
    "\n",
    "    # we don't change eval_env, because we test on the original problem\n",
    "    eval_env.reset(seed=eval_env_seed)\n",
    "    eval_callback = JupyterRenderEvalCallback(\n",
    "        eval_env,\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        n_eval_episodes=20,\n",
    "        n_episodes_to_render=1 if RENDER else 0,\n",
    "        ax=axs[\"render\"],\n",
    "        image=image,\n",
    "    )\n",
    "    plotter.clear()\n",
    "    plotter.set_plt_refs(\n",
    "        axs=[axs[\"return\"], axs[\"success\"]],\n",
    "        lines={\n",
    "            \"eval/mean_reward\": axs[\"return\"].plot([], [], color=color, label=label)[0],\n",
    "            \"eval/success_rate\": axs[\"success\"].plot([], [], color=color)[0],\n",
    "        },\n",
    "    )\n",
    "    axs[\"return\"].legend()\n",
    "\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=3e-3,\n",
    "        gamma=DISCOUNT,\n",
    "        # value function and policy with no hidden layers (i.e. lookup tables)\n",
    "        policy_kwargs=dict(net_arch=dict(pi=[], vf=[])),\n",
    "        device=\"cpu\",\n",
    "        seed=sb3_seed,\n",
    "    )\n",
    "    model.set_logger(logger)\n",
    "    model.learn(\n",
    "        total_timesteps=N_STEPS, log_interval=log_interval, callback=eval_callback\n",
    "    )\n",
    "\n",
    "# clear duplicate plot\n",
    "display.clear_output()\n",
    "\n",
    "save_figure(fig, \"Q2_episodic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Solution:\n",
    "\n",
    "![Solution](figures/E3_Q2.png)\n",
    "\n",
    "With success reward for fix #3 reduced from 130 to 100:\n",
    "\n",
    "![Solution](figures/E3_Q2_fix3-success-reward-100.png)\n",
    "\n",
    "With all rewards scaled up by a factor of 10:\n",
    "\n",
    "![Solution](figures/E3_Q2_reward-scale-10.png)\n",
    "\n",
    "\n",
    "In fact, PPO is indeed learning the optimal policy for the MDP we have defined.\n",
    "The issue is that the MDP we defined does not accurately reflect the task (the success metric) we intended.\n",
    "\n",
    "There are several possible fixes to this problem, either by adjusting the rewards or the termination condition. \n",
    "Implement any two fixes you can think of in the code cell below, then run it.\n",
    "Afterwards, go to the previous code cell and uncomment all entries in the `VARIANTS` list, then run the cell.\n",
    "This will produce a single plot that compares your two solutions to the performance of the original environment.\n",
    "\n",
    "Try to avoid adding rewards with specific values, or rewards that might be too high.\n",
    "Both of these cases may require you to retune your algorithm, and are not necessarily generalizable to more complex problems in 3D.\n",
    "\n",
    "To reduce training time while trying out new ideas, you can increase the `EVAL_INTERVAL`, or just set `RENDER` to `False`, which disables rendering the policy during evaluation.\n",
    "However, remember that visualizing the policy rollouts is essential for gaining insight into why policy learning might be failing.\n",
    "You can also change `MAP_NAME` from `8x8` to `4x4` to prototype ideas on a smaller version of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedFrozenLakeEnv(ShapedRewardsFrozenLakeEnv):\n",
    "    def __init__(self, *args, fix_no=0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fix_no = fix_no\n",
    "\n",
    "    def step(self, a):\n",
    "        obs, reward, terminated, truncated, info = super().step(a)\n",
    "\n",
    "        # modify rewards to be the negative distance to goal\n",
    "        row, col = divmod(obs, self.ncol)\n",
    "        distance = np.linalg.norm([self.goal_row - row, self.goal_col - col], ord=1)\n",
    "        reward = -distance / 10\n",
    "\n",
    "        # add rewards for reaching goal and hitting obstacle\n",
    "        square = self._get_square_type()\n",
    "        if square == b\"G\":\n",
    "            reward += 1\n",
    "        elif square == b\"H\":\n",
    "            reward += -1\n",
    "\n",
    "        if self.fix_no == 1:\n",
    "            ## SOLUTION ##\n",
    "            # convert to infinite-horizon problem\n",
    "            terminated = False\n",
    "            ## END ##\n",
    "        elif self.fix_no == 2:\n",
    "            ## SOLUTION ##\n",
    "            # only success terminates a trajectory\n",
    "            if square == b\"H\":\n",
    "                terminated = False\n",
    "            ## END ##\n",
    "        elif self.fix_no == 3:\n",
    "            # adjust living rewards to disincentivize failure\n",
    "            # living reward should never be negative, even at the starting position\n",
    "            # max(distance) == 14\n",
    "            # -14 / 10 = -1.4\n",
    "            reward += 1.4\n",
    "            if square == b\"G\":\n",
    "                # adjust goal rewards to incentivize success\n",
    "                # reward for success should be greater than reward for standing just\n",
    "                # next to goal square indefinitely\n",
    "                # sum^{\\infty} r \\gamma^t = r / (1 - \\gamma)\n",
    "                # (-1/10 + 1.4) / (1 - 0.99) = 130\n",
    "                reward += 130\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Test Questions (optional)\n",
    "\n",
    "If you have implemented a solution that just retunes the reward values, does your solution still work for different values of $\\gamma$ (`DISCOUNT`)?\n",
    "In particular, try $\\gamma=1.0$.\n",
    "This value of discount is common for episodic tasks in practice, such as robotic manipulation or navigation tasks.\n",
    "\n",
    "Answer: \n",
    "The policy will likely never reach the goal, as this would terminate the episode and stop the flow of (positive) rewards.\n",
    "\n",
    "\n",
    "How does PPO respond to rewards at different scales?\n",
    "For example, what happens if all rewards are increased by a factor of 10?\n",
    "Why might this be happening?\n",
    "\n",
    "Answer: \n",
    "The agent still learns to solve the task, but it learns slower.\n",
    "A fully-featured PPO implementation normalizes the advantages before updating the policy, and some even normalize the rewards themselves, so the effect is somewhat mitigated.\n",
    "However, any function approximator will have more difficulty learning a function with sharp peaks and extreme values than a smoother function (recall that we are using a look-up table for this problem).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Infinite-Horizon Environments (4 points)\n",
    "\n",
    "In this question, we will look at two common infinite-horizon environments.\n",
    "\n",
    "[Inverted Pendulum](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/) is a classic control problem that involves maintaining a vertical pole in an unstable state.\n",
    "It is similar to the [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment, but the actions are continuous rather than discrete.\n",
    "\n",
    "[Reacher](https://gymnasium.farama.org/environments/mujoco/reacher/) is simple continuous problem which involves moving the fingertip of a 2-joint robot to a target position.\n",
    "It's infinite-horizon because it never terminates.\n",
    "The rewards for Reacher are the negative distance to the target position, plus a small control penalty for the norm of the actions applied.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://gymnasium.farama.org/_images/inverted_pendulum.gif\" width=\"300\"/>\n",
    "<img src=\"https://gymnasium.farama.org/_images/reacher.gif\" width=\"300\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3a: Infinite-Horizon Tasks as Episodic Tasks (1 point)\n",
    "\n",
    "Sometimes, it is possible to reformulate an infinite-horizon task as an episodic task while (mostly) preserving the desired behaviour.\n",
    "Let's start by considering what happens if we reformulate Reacher as an episodic task.\n",
    "Episodic Reacher should end if the fingertip is within a threshold of the target position.\n",
    "Fill in a termination condition in the code cell below, then run the cell to compare the infinite-horizon and episodic variants of Reacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.mujoco.reacher_v5 import ReacherEnv\n",
    "\n",
    "TARGET_THRESHOLD = 0.05\n",
    "MAX_VELOCITY = 1.0\n",
    "\n",
    "\n",
    "class EpisodicReacherWrapper(Wrapper):\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        ## SOLUTION ##\n",
    "        # end the trajectory if the fingertip is closer to the target than TARGET_THRESHOLD\n",
    "        # x and y distance from target are final two components of observation\n",
    "        target_vector = observation[8:10]\n",
    "        terminated = np.linalg.norm(target_vector) < TARGET_THRESHOLD\n",
    "        ## END ##\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class ReacherSuccessWrapper(Wrapper):\n",
    "    \"\"\"Adds a success metric to Reacher. A trajectory is successful if it\n",
    "    ends near the target position and fingertip is not moving too fast.\n",
    "    \"\"\"\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        if truncated or terminated:\n",
    "            target_vector = observation[8:10]\n",
    "            ang_velocity = observation[6:8]\n",
    "            success = (\n",
    "                np.linalg.norm(target_vector) < TARGET_THRESHOLD\n",
    "                and (np.abs(ang_velocity) < MAX_VELOCITY).all()\n",
    "            )\n",
    "            info[\"is_success\"] = success\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "def make_reacher(max_episode_steps=50, episodic=False, **kwargs):\n",
    "    env = ReacherEnv(**kwargs)\n",
    "    if episodic:\n",
    "        env = EpisodicReacherWrapper(env)\n",
    "    env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    env = ReacherSuccessWrapper(env)\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "N_STEPS = 50000\n",
    "LOG_FREQ = EVAL_FREQ = 5000\n",
    "RENDER = True\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "sb3_seed = int(rng.integers(2**32))\n",
    "eval_env_seed = int(rng.integers(2**32))\n",
    "\n",
    "# we always evaluate on the original environment, although we might train\n",
    "# on an improved version of the environment\n",
    "eval_env = make_reacher(render_mode=\"rgb_array\")\n",
    "eval_env.reset()  # need to reset before rendering\n",
    "log_interval = LOG_FREQ / eval_env.get_wrapper_attr(\"_max_episode_steps\")\n",
    "\n",
    "fig, axs = create_figure(figsize=(12, 5), n_steps=N_STEPS)\n",
    "fig.suptitle(f\"Q3a: Infinite-Horizon Tasks\")\n",
    "fig.tight_layout()\n",
    "image = axs[\"render\"].imshow(eval_env.render())\n",
    "plotter = PlotFormat(axs=[], lines={})\n",
    "logger = Logger(folder=None, output_formats=[plotter])\n",
    "\n",
    "# NOTE: Comment out any variants you don't want to test\n",
    "VARIANTS = [\n",
    "    (\"infinite-horizon\", False, \"navy\"),\n",
    "    (\"episodic\", True, \"aqua\"),\n",
    "]\n",
    "for label, episodic, color in VARIANTS:\n",
    "\n",
    "    env = make_reacher(episodic=episodic)\n",
    "\n",
    "    # we don't change eval_env, because we test on the original problem\n",
    "    eval_env.reset(seed=eval_env_seed)\n",
    "    eval_callback = JupyterRenderEvalCallback(\n",
    "        eval_env,\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        n_eval_episodes=20,\n",
    "        n_episodes_to_render=1 if RENDER else 0,\n",
    "        ax=axs[\"render\"],\n",
    "        image=image,\n",
    "    )\n",
    "    plotter.clear()\n",
    "    plotter.set_plt_refs(\n",
    "        axs=[axs[\"return\"], axs[\"success\"]],\n",
    "        lines={\n",
    "            \"eval/mean_reward\": axs[\"return\"].plot([], [], color=color, label=label)[0],\n",
    "            \"eval/success_rate\": axs[\"success\"].plot([], [], color=color)[0],\n",
    "        },\n",
    "    )\n",
    "    axs[\"return\"].legend()\n",
    "\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=7.3e-4,\n",
    "        batch_size=256,\n",
    "        gamma=0.98,\n",
    "        tau=0.02,\n",
    "        train_freq=8,\n",
    "        gradient_steps=8,\n",
    "        learning_starts=5000,\n",
    "        policy_kwargs=dict(log_std_init=-3, net_arch=[256, 256]),\n",
    "        seed=sb3_seed,\n",
    "    )\n",
    "    model.set_logger(logger)\n",
    "    model.learn(\n",
    "        total_timesteps=N_STEPS, log_interval=log_interval, callback=eval_callback\n",
    "    )\n",
    "\n",
    "# clear duplicate plot\n",
    "display.clear_output()\n",
    "\n",
    "save_figure(fig, \"Q3a_infinite-horizon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Solution:\n",
    "\n",
    "![Solution](figures/E3_Q3a.png)\n",
    "\n",
    "\n",
    "If you implemented the modification correctly, you should see that the episodic environment results in a similar mean return but a much lower success rate.\n",
    "Although neither environment provides a reward signal directly the success criteria (distance threshold and maximum angular velocity), the agent trained on the infinite-horizon environment learns to fulfill both of them implicitly.\n",
    "In contrast, the agent trained on the episodic environment has no incentive to learn to stop the finger at the target position, since it maximizes the return (during training) by reaching the target position as quickly as possible.\n",
    "Note that both policies are evaluated on the infinite-horizon formulation of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3b: Bounded Returns (2 points)\n",
    "\n",
    "[Inverted Pendulum](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/) is one of the simplest toy problems used in RL.\n",
    "Unfortunately, the default reward function for Inverted Pendulum is not great.\n",
    "For each step that the pole is upright (i.e. for each transition of the trajectory except the last), the reward is $1.0$.\n",
    "For the last transition, the reward is $0.0$.\n",
    "\n",
    "\\begin{equation}\n",
    "    r(s,a,s') = \\mathbf{I}(s' \\notin \\{s_{\\mathrm{failed}}\\})\n",
    "\\end{equation}\n",
    "\n",
    "This might sound like a suitable reward function, as it incentivizes the right thing, namely longer trajectories.\n",
    "For such an **infinite-horizon task**, however, it can be improved upon.\n",
    "\n",
    "\\## SOLUTION ##\n",
    "\n",
    "Explain why the standard reward function for Inverted Pendulum is sub-optimal.\n",
    "\n",
    "In the limiting case as discount $\\gamma \\rightarrow 1.0$, the returns of the optimal policy are unbounded.\n",
    "Each step adds a reward of $1$, and the trajectory never ends, since the policy is optimal and the task has an infinite horizon.\n",
    "Therefore, the return is a diverging infinite sum.\n",
    "\n",
    "In practice, this results in value estimates with very high variance, which results in slow learning of the Q network, and in turn, the policy.\n",
    "Conversely, the Q must still retain enough precision for low value action-state pairs to distinguish between actions that can return the cart to a balanced state and actions that lead to the pole falling over.\n",
    "\n",
    "\\## END ##\n",
    "\n",
    "Luckily, there is a very simple reward transformation that already improves learning.\n",
    "This transformation is completely independent of the state and the action; the new reward is only a function of the old reward.\n",
    "Implement this reward transformation in the wrapper below, and then compare the transformed rewards to the original rewards by running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.mujoco.inverted_pendulum_v5 import InvertedPendulumEnv\n",
    "from gymnasium.wrappers import TransformReward\n",
    "\n",
    "\n",
    "def inverted_pendulum_reward_transform(reward: float) -> float:\n",
    "    ## SOLUTION ##\n",
    "    # implement your fix to the default reward function for InvertedPendulum\n",
    "    reward -= 1.0\n",
    "    ## END ##\n",
    "    return reward\n",
    "\n",
    "\n",
    "class InvertedPendulumSuccessWrapper(Wrapper):\n",
    "    \"\"\"Adds a success metric to InvertedPendulum. Since there is already a failure\n",
    "    condition, we just have to make sure the trajectory timed out instead.\n",
    "    \"\"\"\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        if truncated or terminated:\n",
    "            info[\"is_success\"] = truncated\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "def make_inverted_pendulum(max_episode_steps=200, fix_rewards=False, **kwargs):\n",
    "    env = InvertedPendulumEnv(**kwargs)\n",
    "    if fix_rewards:\n",
    "        env = TransformReward(env, inverted_pendulum_reward_transform)\n",
    "    env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    env = InvertedPendulumSuccessWrapper(env)\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "N_STEPS = 10000\n",
    "LOG_FREQ = EVAL_FREQ = 1000\n",
    "RENDER = True\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "sb3_seed = int(rng.integers(2**32))\n",
    "eval_env_seed = int(rng.integers(2**32))\n",
    "\n",
    "# we always evaluate on the original environment, although we might train\n",
    "# on an improved version of the environment\n",
    "eval_env = make_inverted_pendulum(render_mode=\"rgb_array\")\n",
    "eval_env.reset()  # need to reset before rendering\n",
    "log_interval = LOG_FREQ / eval_env.get_wrapper_attr(\"_max_episode_steps\")\n",
    "\n",
    "fig, axs = create_figure(figsize=(12, 5), n_steps=N_STEPS)\n",
    "fig.suptitle(f\"Q3b: Bounded Returns\")\n",
    "fig.tight_layout()\n",
    "image = axs[\"render\"].imshow(eval_env.render())\n",
    "plotter = PlotFormat(axs=[], lines={})\n",
    "logger = Logger(folder=None, output_formats=[plotter])\n",
    "\n",
    "# NOTE: Comment out any variants you don't want to test\n",
    "VARIANTS = [\n",
    "    (\"original rewards\", False, \"red\"),\n",
    "    (\"modified rewards\", True, \"navy\"),\n",
    "]\n",
    "for label, fix_rewards, color in VARIANTS:\n",
    "\n",
    "    env = make_inverted_pendulum(fix_rewards=fix_rewards)\n",
    "\n",
    "    # we don't change eval_env, because we test on the original problem\n",
    "    eval_env.reset(seed=eval_env_seed)\n",
    "    eval_callback = JupyterRenderEvalCallback(\n",
    "        eval_env,\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        n_eval_episodes=20,\n",
    "        n_episodes_to_render=1 if RENDER else 0,\n",
    "        ax=axs[\"render\"],\n",
    "        image=image,\n",
    "    )\n",
    "    plotter.clear()\n",
    "    plotter.set_plt_refs(\n",
    "        axs=[axs[\"return\"], axs[\"success\"]],\n",
    "        lines={\n",
    "            \"eval/mean_reward\": axs[\"return\"].plot([], [], color=color, label=label)[0],\n",
    "            \"eval/success_rate\": axs[\"success\"].plot([], [], color=color)[0],\n",
    "        },\n",
    "    )\n",
    "    axs[\"return\"].legend()\n",
    "\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        seed=sb3_seed,\n",
    "    )\n",
    "    model.set_logger(logger)\n",
    "    model.learn(\n",
    "        total_timesteps=N_STEPS, log_interval=log_interval, callback=eval_callback\n",
    "    )\n",
    "\n",
    "# clear duplicate plot\n",
    "display.clear_output()\n",
    "\n",
    "save_figure(fig, \"Q3b_bounded_returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Solution:\n",
    "\n",
    "![Solution](figures/E3_Q3b.png)\n",
    "\n",
    "Using PPO instead of SAC:\n",
    "\n",
    "![Solution](figures/E3_Q3b_ppo.png)\n",
    "\n",
    "\n",
    "### Self-Test Questions (optional)\n",
    "\n",
    "In this question, we used SAC to demonstrate the effect that these two (equivalent) reward functions can have.\n",
    "Empirically, PPO is less affected by the choice between these two reward functions.\n",
    "Can you explain why?\n",
    "\n",
    "Answer: \n",
    "PPO uses Generalized Advantage Estimation (GAE) to compute returns, which combines rewards with value estimates at each time step.\n",
    "The lambda hyperparameter of GAE acts as a multiplier to the discount, which also mitigates this effect.\n",
    "Furthermore, advantages are normalized before updating the policy, reducing the effect of extreme returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3c: Discount and the Pull of Time (1 point)\n",
    "\n",
    "In the previous question, we considered the limiting case when $\\gamma \\rightarrow 1.0$ to gain insight into the reward function.\n",
    "This limiting case is unfavourable for the original reward function of Inverted Pendulum, but setting $\\gamma = 1.0$ is actually suboptimal for most infinite-horizon tasks.\n",
    "Even the \"fixed\" reward function suffers under this condition.\n",
    "\n",
    "Although the discount $\\gamma$ is technically part of the MDP definition, most frameworks treat it as a hyperparameter of the algorithm.\n",
    "In the code cell below, in the for loop, set `gamma` in the learning pipeline according to the value of the loop variable `gamma`.\n",
    "Then run the code cell to compare the effect of these two values of discount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 100000\n",
    "LOG_FREQ = EVAL_FREQ = 10000\n",
    "RENDER = True\n",
    "\n",
    "\n",
    "env = make_inverted_pendulum(fix_rewards=True)\n",
    "# we always evaluate on the original environment, although we might train\n",
    "# on an improved version of the environment\n",
    "eval_env = make_inverted_pendulum(render_mode=\"rgb_array\")\n",
    "log_interval = LOG_FREQ / eval_env.get_wrapper_attr(\"_max_episode_steps\")\n",
    "\n",
    "fig, axs = create_figure(figsize=(12, 5), n_steps=N_STEPS)\n",
    "fig.suptitle(f\"Q3c: Discount\")\n",
    "fig.tight_layout()\n",
    "image = axs[\"render\"].imshow(eval_env.render())\n",
    "plotter = PlotFormat(axs=[], lines={})\n",
    "logger = Logger(folder=None, output_formats=[plotter])\n",
    "\n",
    "# NOTE: Comment out any variants you don't want to test\n",
    "VARIANTS = [\n",
    "    (\"modified rewards\", 0.99, \"navy\"),\n",
    "    (\"modified reward + discount=1\", 1.0, \"red\"),\n",
    "]\n",
    "for label, gamma, color in VARIANTS:\n",
    "\n",
    "    eval_env.reset(seed=eval_env_seed)\n",
    "    eval_callback = JupyterRenderEvalCallback(\n",
    "        eval_env,\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        n_eval_episodes=20,\n",
    "        n_episodes_to_render=1 if RENDER else 0,\n",
    "        ax=axs[\"render\"],\n",
    "        image=image,\n",
    "    )\n",
    "    plotter.clear()\n",
    "    plotter.set_plt_refs(\n",
    "        axs=[axs[\"return\"], axs[\"success\"]],\n",
    "        lines={\n",
    "            \"eval/mean_reward\": axs[\"return\"].plot([], [], color=color, label=label)[0],\n",
    "            \"eval/success_rate\": axs[\"success\"].plot([], [], color=color)[0],\n",
    "        },\n",
    "    )\n",
    "    axs[\"return\"].legend()\n",
    "\n",
    "    ## SOLUTION ##\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        gamma=gamma,\n",
    "        gae_lambda=1.0,  # use monte-carlo returns\n",
    "        device=\"cpu\",\n",
    "        seed=sb3_seed,\n",
    "    )\n",
    "    model.set_logger(logger)\n",
    "    model.learn(\n",
    "        total_timesteps=N_STEPS, log_interval=log_interval, callback=eval_callback\n",
    "    )\n",
    "    ## END ##\n",
    "\n",
    "# clear duplicate plot\n",
    "display.clear_output()\n",
    "\n",
    "save_figure(fig, \"Q3c_discount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Solution:\n",
    "\n",
    "![Solution](figures/E3_Q3c.png)\n",
    "\n",
    "With original rewards and `discount=1.0`:\n",
    "\n",
    "![Solution](figures/E3_Q3c_original-rewards.png)\n",
    "\n",
    "\n",
    "### Self-Test Questions (optional)\n",
    "\n",
    "In this question, we used PPO to demonstrate the effect of the discount factor.\n",
    "Algorithms that rely on Q-Learning, such as SAC and DQN, are less affected by the choice of discount factor.\n",
    "Can you explain why?\n",
    "\n",
    "Answer: \n",
    "Q-Learning and SAC only consider single-step transitions, where the discount is only used to slightly reduce the value estimate of the next state.\n",
    "The effect of the discount is therefore much smaller than the effect of the Q function estimate itself.\n",
    "In contrast, PPO optimizes its policy using advantages, which are computed from longer sequences of actions, such that the discount has a larger effect.\n",
    "\n",
    "\n",
    "We set a value called `gae_lambda` ($\\lambda_{GAE}$) to $1.0$ in this question.\n",
    "What does this hyperparameter do?\n",
    "Why does setting it to $1.0$ make it easier to demonstrate the effect of the discount factor $\\gamma$?\n",
    "\n",
    "Answer: \n",
    "Generalized Advantage Estimation (GAE) mixes the reward values with value estimates from each step.\n",
    "Setting `gae_lambda=1.0`, disables GAE and instead uses the monte carlo estimate to compute returns (the monte carlo estimate is the true observed return from a trajectory).\n",
    "Without this change, GAE would result in differing return estimates even if the underlying return is always the same, which would increase the learning signal to the agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Multi-Phase Tasks (6 points)\n",
    "\n",
    "In the real world, the vast majority of interesting tasks can be categorized as multi-phase tasks.\n",
    "A multi-phase task is any task where the objective or reward function shifts during the trajectory.\n",
    "For example, think of picking up a cup and placing it somewhere else, or inserting a key into a lock and then turning it.\n",
    "The first phase of the desired motion probably requires a different shaped reward than the second phase.\n",
    "Formally, we can define a reward function for the task that depends on the phase, but colloquially we usually speak about separate reward functions for each phase.\n",
    "The phase may be observable or it may need to be inferred from the observable features of the environment.\n",
    "\n",
    "Multi-phase MDP design is hard, with many pitfalls.\n",
    "We have implemented a multi-phase version of the Reacher environment, in which if the finger is within a threshold of the target, a new target is sampled.\n",
    "The finger must not be moving too quickly for the next phase to be triggered.\n",
    "After the target is reached in the second phase, the task is successful.\n",
    "\n",
    "Unfortunately, the algorithm (SAC with well-tuned hyperparameters) cannot learn a successful policy from our naive environment.\n",
    "Find a way to modify the environment given in `FixedMultiPhaseReacherEnv` so that the algorithm learns to solve the task.\n",
    "You can modify the observations or anything in the `step` function.\n",
    "The `phase_success` variable is pre-computed for you in a way that is still correct even if the observations change, and probably doesn't need to be changed.\n",
    "\n",
    "Consider the task and its rewards from the agent's perspective throughout the desired trajectory.\n",
    "Again, try to avoid adding rewards with specific values, or rewards that might be too high.\n",
    "Both of these cases may require you to retune your algorithm, and are not necessarily generalizable to more complex problems in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium.spaces as spaces\n",
    "from gymnasium.envs.mujoco.reacher_v5 import ReacherEnv\n",
    "\n",
    "TARGET_THRESHOLD = 0.05\n",
    "MAX_VELOCITY = 1.0\n",
    "\n",
    "\n",
    "class MultiPhaseReacherEnv(ReacherEnv):\n",
    "    def __init__(self, *args, n_phases=2, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_phases = n_phases  # Two phases for two targets\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        # Check if the current target is reached and the arm is not moving too fast\n",
    "        target_vector = observation[8:10]\n",
    "        ang_velocity = observation[6:8]\n",
    "        phase_success = (\n",
    "            np.linalg.norm(target_vector) < TARGET_THRESHOLD\n",
    "            and (np.abs(ang_velocity) < MAX_VELOCITY).all()\n",
    "        )\n",
    "\n",
    "        if phase_success and self.current_phase < (self.n_phases - 1):\n",
    "            # increment phase\n",
    "            self.current_phase += 1\n",
    "\n",
    "            # set new target and get new observation, since it depends on target\n",
    "            self._set_new_target()\n",
    "            observation = self._get_obs()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset_model(self):\n",
    "        self.current_phase = 0\n",
    "        return super().reset_model()\n",
    "\n",
    "    def _set_new_target(self):\n",
    "        while True:\n",
    "            target = self.np_random.uniform(low=-0.2, high=0.2, size=2)\n",
    "            if np.linalg.norm(target) < 0.2:\n",
    "                break\n",
    "\n",
    "        qpos = self.data.qpos.copy()\n",
    "        qpos[-2:] = target\n",
    "        self.set_state(qpos, self.data.qvel)\n",
    "\n",
    "\n",
    "class MultiPhaseReacherSuccessWrapper(Wrapper):\n",
    "    \"\"\"Adds a success metric to Reacher. A trajectory is successful if it\n",
    "    ends near the target position and fingertip is not moving too fast.\n",
    "    \"\"\"\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        if truncated or terminated:\n",
    "            # ensure we get unmodified observation of parent environment\n",
    "            base_obs = ReacherEnv._get_obs(self.env.unwrapped)\n",
    "            target_vector = base_obs[8:10]\n",
    "            ang_velocity = base_obs[6:8]\n",
    "            phase = self.env.unwrapped.current_phase\n",
    "            success = (\n",
    "                np.linalg.norm(target_vector) < TARGET_THRESHOLD\n",
    "                and (np.abs(ang_velocity) < MAX_VELOCITY).all()\n",
    "                and phase == (self.env.unwrapped.n_phases - 1)\n",
    "            )\n",
    "            info[\"is_success\"] = success\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class FixedMultiPhaseReacherEnv(MultiPhaseReacherEnv):\n",
    "    ## SOLUTION ##\n",
    "    # Note: Obviously there are many solutions, this is just one example.\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # CHANGED: shape of observation must be defined correctly in the observation space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(11,), dtype=np.float64\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        # CHANGED: decrease reward in first phase to ensure reward always\n",
    "        # increases as phase increases\n",
    "        reward -= (self.n_phases - self.current_phase - 1) * 1.0\n",
    "\n",
    "        # Check if the current target is reached and the arm is not moving too fast\n",
    "        # ensure we get unmodified observation of parent environment\n",
    "        base_obs = ReacherEnv._get_obs(self)\n",
    "        target_vector = base_obs[8:10]\n",
    "        ang_velocity = base_obs[6:8]\n",
    "        phase_success = (\n",
    "            np.linalg.norm(target_vector) < TARGET_THRESHOLD\n",
    "            and (np.abs(ang_velocity) < MAX_VELOCITY).all()\n",
    "        )\n",
    "\n",
    "        if phase_success and self.current_phase < (self.n_phases - 1):\n",
    "            # increment phase\n",
    "            self.current_phase += 1\n",
    "\n",
    "            # CHANGED: terminate after final phase success\n",
    "            if self.current_phase == (self.n_phases - 1):\n",
    "                terminated = True\n",
    "\n",
    "            # set new target and get new observation, since it depends on target\n",
    "            self._set_new_target()\n",
    "            observation = self._get_obs()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        observation = super()._get_obs()\n",
    "\n",
    "        # CHANGED: add the phase to the observation\n",
    "        # the return changes with phase\n",
    "        observation = np.concatenate([observation, [self.current_phase]])\n",
    "\n",
    "        return observation\n",
    "\n",
    "    ## END ##\n",
    "\n",
    "\n",
    "def make_multiphasereacher(\n",
    "    n_phases=2, with_fixes=False, max_episode_steps=100, **kwargs\n",
    "):\n",
    "    EnvCls = FixedMultiPhaseReacherEnv if with_fixes else MultiPhaseReacherEnv\n",
    "    env = EnvCls(n_phases=n_phases, **kwargs)\n",
    "    env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    env = MultiPhaseReacherSuccessWrapper(env)\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "N_STEPS = 80_000\n",
    "LOG_FREQ = EVAL_FREQ = 4000\n",
    "RENDER = True\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "sb3_seed = int(rng.integers(2**32))\n",
    "eval_env_seed = int(rng.integers(2**32))\n",
    "\n",
    "fig, axs = create_figure(figsize=(12, 5), n_steps=N_STEPS)\n",
    "fig.suptitle(f\"Q4: Multi-Phase Reacher\")\n",
    "fig.tight_layout()\n",
    "plotter = PlotFormat(axs=[], lines={})\n",
    "logger = Logger(folder=None, output_formats=[plotter])\n",
    "\n",
    "# NOTE: Comment out any variants you don't want to test\n",
    "VARIANTS = [\n",
    "    (\"without fixes\", False, \"red\"),\n",
    "    (\"fixed\", True, \"navy\"),\n",
    "]\n",
    "for label, with_fixes, color in VARIANTS:\n",
    "\n",
    "    env = make_multiphasereacher(with_fixes=with_fixes)\n",
    "\n",
    "    # we evaluate on the training environment, because the observations might change\n",
    "    eval_env = make_multiphasereacher(with_fixes=with_fixes, render_mode=\"rgb_array\")\n",
    "    eval_env.reset(seed=eval_env_seed)\n",
    "    log_interval = LOG_FREQ / eval_env.get_wrapper_attr(\"_max_episode_steps\")\n",
    "    eval_callback = JupyterRenderEvalCallback(\n",
    "        eval_env,\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        n_eval_episodes=20,\n",
    "        n_episodes_to_render=1,\n",
    "        ax=axs[\"render\"],\n",
    "        image=axs[\"render\"].imshow(eval_env.render()),\n",
    "    )\n",
    "    plotter.clear()\n",
    "    plotter.set_plt_refs(\n",
    "        axs=[axs[\"return\"], axs[\"success\"]],\n",
    "        lines={\n",
    "            \"eval/mean_reward\": axs[\"return\"].plot([], [], color=color, label=label)[0],\n",
    "            \"eval/success_rate\": axs[\"success\"].plot([], [], color=color)[0],\n",
    "        },\n",
    "    )\n",
    "    axs[\"return\"].legend()\n",
    "\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=7.3e-4,\n",
    "        batch_size=256,\n",
    "        gamma=0.98,\n",
    "        tau=0.02,\n",
    "        train_freq=8,\n",
    "        gradient_steps=8,\n",
    "        learning_starts=5000,\n",
    "        policy_kwargs=dict(log_std_init=-3, net_arch=[256, 256]),\n",
    "        seed=sb3_seed,\n",
    "    )\n",
    "    model.set_logger(logger)\n",
    "    model.learn(\n",
    "        total_timesteps=N_STEPS, log_interval=log_interval, callback=eval_callback\n",
    "    )\n",
    "\n",
    "# clear duplicate plot\n",
    "display.clear_output()\n",
    "\n",
    "save_figure(fig, \"Q4_multiphase_reacher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Solution:\n",
    "\n",
    "![Solution](figures/E3_Q4.png)\n",
    "\n",
    "\n",
    "This page is intentionally left blank."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eQx7oDGeeKWj"
   ],
   "name": "2_dqn_atari.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
